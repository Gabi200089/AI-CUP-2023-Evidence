{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b47d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: ipywidgets==8.0.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 3)) (8.0.5)\n",
      "Requirement already satisfied: pandarallel==1.6.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 4)) (1.6.4)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: tensorboard==2.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 7)) (2.11.0)\n",
      "Requirement already satisfied: torch==1.13.1+cu116 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 8)) (1.13.1+cu116)\n",
      "Requirement already satisfied: tqdm==4.65.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 9)) (4.65.0)\n",
      "Requirement already satisfied: opencc in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 10)) (1.1.1)\n",
      "Requirement already satisfied: hanlp in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 11)) (2.1.0b49)\n",
      "Requirement already satisfied: transformers==4.27.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 12)) (4.27.1)\n",
      "Requirement already satisfied: wikipedia==1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 13)) (1.4.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets==8.0.5->-r requirements.txt (line 3)) (8.11.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets==8.0.5->-r requirements.txt (line 3)) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets==8.0.5->-r requirements.txt (line 3)) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets==8.0.5->-r requirements.txt (line 3)) (3.0.5)\n",
      "Requirement already satisfied: dill>=0.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandarallel==1.6.4->-r requirements.txt (line 4)) (0.3.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandarallel==1.6.4->-r requirements.txt (line 4)) (5.9.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (1.23.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 6)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (1.51.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (2.16.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (3.4.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (2.28.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (63.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (0.40.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==1.13.1+cu116->-r requirements.txt (line 8)) (4.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm==4.65.0->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (0.11.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wikipedia==1.4.0->-r requirements.txt (line 13)) (4.11.2)\n",
      "Requirement already satisfied: hanlp-common>=0.0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (0.0.19)\n",
      "Requirement already satisfied: hanlp-downloader in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (0.0.25)\n",
      "Requirement already satisfied: hanlp-trie>=0.0.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (0.0.5)\n",
      "Requirement already satisfied: pynvml in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (11.5.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.91 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (0.1.99)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (2.2.0)\n",
      "Requirement already satisfied: toposort==1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (1.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11.0->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: phrasetree in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hanlp-common>=0.0.19->hanlp->-r requirements.txt (line 11)) (0.0.8)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1->-r requirements.txt (line 12)) (2023.5.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard==2.11.0->-r requirements.txt (line 7)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard==2.11.0->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard==2.11.0->-r requirements.txt (line 7)) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard==2.11.0->-r requirements.txt (line 7)) (2022.9.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard==2.11.0->-r requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->wikipedia==1.4.0->-r requirements.txt (line 13)) (2.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11.0->-r requirements.txt (line 7)) (3.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ce24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# 3rd party libs\n",
    "import hanlp\n",
    "import opencc\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from hanlp.components.pipeline import Pipeline\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# our own libs\n",
    "from utils import load_json\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
    "wikipedia.set_lang(\"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e74b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1. Document retrieval\n",
    "\n",
    "# Step 1:使用Constituency Parser 找出 claim 中的的 Noun Phrases(NPs)\n",
    "# Step 2:從Ｗikipedia API 中取出中取出 NP相符合的頁面名稱\n",
    "# Step 3:保留出現在句子index最靠前的五篇文章作為相關文章\n",
    "# Prepare the environment and import all library we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac1ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 讀檔案\n",
    "# import jsonlines\n",
    "\n",
    "# file_path = 'data/merged.jsonl'\n",
    "\n",
    "# with jsonlines.open(file_path, 'r') as file:\n",
    "#     for i, item in enumerate(file.iter()):\n",
    "#         print(item)\n",
    "#         if i == 4: \n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04cdf1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = load_json(\"data/merge_train.jsonl\")\n",
    "TEST_DATA = load_json(\"data/merge_test_data.jsonl\")\n",
    "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
    "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "589a4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Claim:\n",
    "    data: str\n",
    "\n",
    "@dataclass\n",
    "class AnnotationID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class EvidenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class PageTitle:\n",
    "    title: str\n",
    "\n",
    "@dataclass\n",
    "class SentenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class Evidence:\n",
    "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8ea466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將繁體中文轉成簡體中文\n",
    "def do_st_corrections(text: str) -> str:\n",
    "    simplified = CONVERTER_T2S.convert(text)\n",
    "    return CONVERTER_S2T.convert(simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f04a45a-1b36-445b-a6f7-cbb16e7a7aad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hanlp_restful in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.0.23)\n",
      "Requirement already satisfied: hanlp-common in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from hanlp_restful) (0.0.19)\n",
      "Requirement already satisfied: phrasetree in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from hanlp-common->hanlp_restful) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install hanlp_restful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be780b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以此提取句子中的名詞\n",
    "def get_nps_hanlp(\n",
    "    #pedictor 為HanLP的套件預測器，用來預測給定文本的語法樹，排序句子中詞的優先程度\n",
    "    predictor: Pipeline,\n",
    "    d: Dict[str, Union[int, Claim, Evidence]],\n",
    ") -> List[str]:\n",
    "    claim = d[\"claim\"]\n",
    "    tree = predictor(claim)[\"con\"]\n",
    "    nps = [\n",
    "        #將claim宣稱的資料從繁體轉為簡體\n",
    "        do_st_corrections(\"\".join(subtree.leaves()))\n",
    "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
    "    ]\n",
    "    #print(nps)\n",
    "    return nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e92fcc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#優化提取函式\n",
    "# def get_nps_hanlp(\n",
    "#     predictor: Pipeline,\n",
    "#     d: Dict[str, Union[int, Claim, Evidence]],\n",
    "# ) -> Generator[str, None, None]:\n",
    "#     claim = d[\"claim\"]\n",
    "#     tree = predictor(claim)[\"con\"]\n",
    "\n",
    "#     leaves = []\n",
    "\n",
    "#     for subtree in tree.subtrees(lambda t: t.label() == \"NP\"):\n",
    "#         if not leaves:\n",
    "#             leaves = list(subtree.leaves())\n",
    "#         else:\n",
    "#             leaves.extend(subtree.leaves())\n",
    "\n",
    "#     nps = do_st_corrections(\"\".join(leaves)) if leaves else None\n",
    "\n",
    "#     yield nps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da5ec913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要要看招回率是否足夠高\n",
    "def calculate_precision(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    ") -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        if len(predicted_pages) != 0:\n",
    "            precision += len(hits) / len(predicted_pages)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Macro precision\n",
    "    print(f\"Precision: {precision / count}\")\n",
    "\n",
    "\n",
    "def calculate_recall(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    ") -> None:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        recall += len(hits) / len(gt_pages)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Recall: {recall / count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2351e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預設檢索的文件數量最多為五份。根據您的目標，可以調整這個num_pred_doc數量。\n",
    "# 以jsonl格式保存數據。\n",
    "def save_doc(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    "    mode: str = \"train\",\n",
    "    num_pred_doc: int = 5,\n",
    ") -> None:\n",
    "    with open(\n",
    "        f\"data/{mode}_doc{num_pred_doc}.jsonl\",\n",
    "        \"w\",\n",
    "        encoding=\"utf8\",\n",
    "    ) as f:\n",
    "        for i, d in enumerate(data):\n",
    "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f27ca318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要文件檢索功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67eee2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_pages(series_data: pd.Series) -> Set[Dict[int, str]]:\n",
    "    import wikipedia\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    import opencc\n",
    "    import re\n",
    "    wikipedia.set_lang('zh')\n",
    "    CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
    "    CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
    "    def do_st_corrections(text: str) -> str:\n",
    "        simplified = CONVERTER_T2S.convert(text)\n",
    "        return CONVERTER_S2T.convert(simplified)\n",
    "    \n",
    "    results = []\n",
    "    tmp_muji = []\n",
    "    # wiki_page: its index showned in claim\n",
    "    mapping = {}\n",
    "    claim = series_data[\"claim\"]\n",
    "    nps = series_data[\"hanlp_results\"]\n",
    "    first_wiki_term = []\n",
    "\n",
    "    for i, np in enumerate(nps):\n",
    "        # Simplified Traditional Chinese Correction\n",
    "        wiki_search_results = [\n",
    "            do_st_corrections(w) for w in wikipedia.search(np)\n",
    "        ]\n",
    "\n",
    "        # Remove the wiki page's description in brackets\n",
    "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
    "        wiki_df = pd.DataFrame({\n",
    "            \"wiki_set\": wiki_set,\n",
    "            \"wiki_results\": wiki_search_results\n",
    "        })\n",
    "\n",
    "        # Elements in wiki_set --> index\n",
    "        # Extracting only the first element is one way to avoid extracting\n",
    "        # too many of the similar wiki pages\n",
    "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
    "        candidates = grouped_df[\"wiki_results\"].tolist()\n",
    "        # muji refers to wiki_set\n",
    "        muji = grouped_df.index.tolist()\n",
    "\n",
    "        for prefix, term in zip(muji, candidates):\n",
    "            if prefix not in tmp_muji:\n",
    "                matched = False\n",
    "\n",
    "                # Take at least one term from the first noun phrase\n",
    "                if i == 0:\n",
    "                    first_wiki_term.append(term)\n",
    "\n",
    "                # Walrus operator :=\n",
    "                # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
    "                # Through these filters, we are trying to figure out if the term\n",
    "                # is within the claim\n",
    "                if (((new_term := term) in claim) or\n",
    "                    ((new_term := term.replace(\"·\", \"\")) in claim) or\n",
    "                    ((new_term := term.split(\" \")[0]) in claim) or\n",
    "                    ((new_term := term.replace(\"-\", \" \")) in claim)):\n",
    "                    matched = True\n",
    "\n",
    "                elif \"·\" in term:\n",
    "                    splitted = term.split(\"·\")\n",
    "                    for split in splitted:\n",
    "                        if (new_term := split) in claim:\n",
    "                            matched = True\n",
    "                            break\n",
    "\n",
    "                if matched:\n",
    "                    # post-processing\n",
    "                    term = term.replace(\" \", \"_\")\n",
    "                    term = term.replace(\"-\", \"\")\n",
    "                    results.append(term)\n",
    "                    mapping[term] = claim.find(new_term)\n",
    "                    tmp_muji.append(new_term)\n",
    "\n",
    "    # 5 is a hyperparameter\n",
    "    if len(results) > 5:\n",
    "        assert -1 not in mapping.values()\n",
    "        results = sorted(mapping, key=mapping.get)[:5]\n",
    "    elif len(results) < 1:\n",
    "        results = first_wiki_term\n",
    "\n",
    "    return set(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eef688ca-0322-407c-a88f-2a6a998c16da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import wikipedia\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import opencc\n",
    "# import re\n",
    "# from typing import List, Dict, Set\n",
    "\n",
    "# def get_pred_pages(series_data: pd.Series) -> Set[Dict[int, str]]:\n",
    "#     wikipedia.set_lang('zh')\n",
    "#     CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
    "#     CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
    "\n",
    "#     def do_st_corrections(text: str) -> str:\n",
    "#         simplified = CONVERTER_T2S.convert(text)\n",
    "#         return CONVERTER_S2T.convert(simplified)\n",
    "\n",
    "#     def get_filtered_term(term: str, claim: str) -> str:\n",
    "#         new_term = term\n",
    "#         if (new_term := term) in claim or \\\n",
    "#                 (new_term := term.replace(\"·\", \"\")) in claim or \\\n",
    "#                 (new_term := term.split(\" \")[0]) in claim or \\\n",
    "#                 (new_term := term.replace(\"-\", \" \")) in claim:\n",
    "#             return new_term\n",
    "#         elif \"·\" in term:\n",
    "#             splitted = term.split(\"·\")\n",
    "#             for split in splitted:\n",
    "#                 if (new_term := split) in claim:\n",
    "#                     return new_term\n",
    "#         return \"\"\n",
    "\n",
    "#     results = []\n",
    "#     tmp_muji = []\n",
    "#     mapping = {}\n",
    "#     claim = series_data[\"claim\"]\n",
    "#     nps = series_data[\"hanlp_results\"]\n",
    "#     first_wiki_term = []\n",
    "\n",
    "#     for i, np in enumerate(nps):\n",
    "#         wiki_search_results = [do_st_corrections(w) for w in wikipedia.search(np)]\n",
    "#         wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
    "#         wiki_df = pd.DataFrame({\"wiki_set\": wiki_set, \"wiki_results\": wiki_search_results})\n",
    "#         grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
    "#         candidates = grouped_df[\"wiki_results\"].tolist()\n",
    "#         muji = grouped_df.index.tolist()\n",
    "\n",
    "#         for prefix, term in zip(muji, candidates):\n",
    "#             if prefix not in tmp_muji:\n",
    "#                 matched = False\n",
    "#                 new_term = get_filtered_term(term, claim)\n",
    "#                 if new_term:\n",
    "#                     matched = True\n",
    "#                 if matched:\n",
    "#                     term = term.replace(\" \", \"_\").replace(\"-\", \"\")\n",
    "#                     results.append(term)\n",
    "#                     mapping[term] = claim.find(new_term)\n",
    "#                     tmp_muji.append(new_term)\n",
    "\n",
    "#     if len(results) > 5:\n",
    "#         assert -1 not in mapping.values()\n",
    "#         results = sorted(mapping, key=mapping.get)[:5]\n",
    "#     elif len(results) < 1:\n",
    "#         results = first_wiki_term\n",
    "\n",
    "#     return set(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df89fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加載完成\n"
     ]
    }
   ],
   "source": [
    "# 第一步：從HanLP的結構解析樹中獲取名詞片語\n",
    "# 設置HanLP預測器（1分鐘）\n",
    "# HanLP：面向生產環境的前沿多語種自然語言處理技術\n",
    "predictor = (hanlp.pipeline().append(\n",
    "    #原先模型FINE_ELECTRA_SMALL_ZH\n",
    "    #https://hanlp.hankcs.com/docs/api/hanlp/pretrained/tok.html\n",
    "    hanlp.load(\"MSR_TOK_ELECTRA_BASE_CRF\"),\n",
    "    output_key=\"tok\",\n",
    ").append(\n",
    "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
    "    output_key=\"con\",\n",
    "    input_key=\"tok\",\n",
    "))\n",
    "print('加載完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61c16475",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanlp_file = f\"data/hanlp_con_results.pkl\"\n",
    "# 如果hanlp_file存在的話，將會直接開啟原本存在的檔案\n",
    "if Path(hanlp_file).exists():\n",
    "    with open(hanlp_file, \"rb\") as f:\n",
    "        hanlp_results = pickle.load(f)\n",
    "else:\n",
    "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TRAIN_DATA]\n",
    "    with open(hanlp_file, \"wb\") as f:\n",
    "        pickle.dump(hanlp_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5ddf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = f\"data/train_doc5.jsonl\"\n",
    "# 同上\n",
    "if Path(doc_path).exists():\n",
    "    with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "        predicted_results = pd.Series([\n",
    "            set(json.loads(line)[\"predicted_pages\"])\n",
    "            for line in f\n",
    "        ])\n",
    "else:\n",
    "    train_df = pd.DataFrame(TRAIN_DATA)\n",
    "    train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
    "    predicted_results = train_df.parallel_apply(get_pred_pages, axis=1)\n",
    "    save_doc(TRAIN_DATA, predicted_results, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c24c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.246152922818402\n",
      "Recall: 0.8314037930620573\n"
     ]
    }
   ],
   "source": [
    "# 計算 precision 和 recall\n",
    "calculate_precision(TRAIN_DATA, predicted_results)\n",
    "calculate_recall(TRAIN_DATA, predicted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c5a6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11647\n",
      "11647\n"
     ]
    }
   ],
   "source": [
    "# 確保兩者長度相同\n",
    "print(len(TRAIN_DATA))\n",
    "print(len(predicted_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be04e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanlp_test_file = f\"data/hanlp_con_test_results.pkl\"\n",
    "# 同上\n",
    "if Path(hanlp_test_file).exists():\n",
    "    with open(hanlp_file, \"rb\") as f:\n",
    "        hanlp_results = pickle.load(f)\n",
    "else:\n",
    "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA]\n",
    "    with open(hanlp_file, \"wb\") as f:\n",
    "        pickle.dump(hanlp_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8761517",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_path = f\"data/test_doc5.jsonl\"\n",
    "# 同上\n",
    "if Path(test_doc_path).exists():\n",
    "    with open(test_doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "        test_results = pd.Series(\n",
    "            [set(json.loads(line)[\"predicted_pages\"]) for line in f])\n",
    "else:\n",
    "    test_df = pd.DataFrame(TEST_DATA)\n",
    "    test_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
    "    test_results = test_df.parallel_apply(get_pred_pages, axis=1)\n",
    "    save_doc(TEST_DATA, test_results, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a14a02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2. Sentence retrieval\n",
    "\n",
    "# Step1:從前一步驟找出的相關文章，再進一步抽取出相關句子作為證據句\n",
    "# Step2:將claim與句子丟入BERT，訓練它做二分類，判斷「證據句/非證據句」\n",
    "# built-in libs\n",
    "# import 需要的libs\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# third-party libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset, Dataset\n",
    "\n",
    "# local libs\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afd596b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置全域變數 Global variable\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/merge_train.jsonl\")\n",
    "TEST_DATA = load_json(\"data/merge_test_data.jsonl\")\n",
    "DOC_DATA = load_json(\"data/train_doc5.jsonl\")\n",
    "\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
    "# GT means Ground Truth\n",
    "TRAIN_GT, DEV_GT = train_test_split(\n",
    "    DOC_DATA,\n",
    "# 0.2\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    shuffle=True,\n",
    "    stratify=_y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2acdb067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b4784f3be24426be8fb68a0a50230c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118776), Label(value='0 / 118776')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "# 預先載入wikipedia的資料庫\n",
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "del wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "409e9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輔助函式\n",
    "# 計算句子檢索的精確度 precision\n",
    "\n",
    "def evidence_macro_precision(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate precision for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of precision)\n",
    "        [2]: retrieved (denominator of precision)\n",
    "    \"\"\"\n",
    "    this_precision = 0.0\n",
    "    this_precision_hits = 0.0\n",
    "    # Return 0, 0 if label is not enough info since not enough info does not\n",
    "    # contain any evidence.\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # e[2] is the page title, e[3] is the sentence index\n",
    "        all_evi = [[e[2], e[3]]\n",
    "                   for eg in instance[\"evidence\"]\n",
    "                   for e in eg\n",
    "                   if e[3] is not None]\n",
    "        claim = instance[\"claim\"]\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "        for prediction in predicted_evidence:\n",
    "            if prediction in all_evi:\n",
    "                this_precision += 1.0\n",
    "            this_precision_hits += 1.0\n",
    "        return (this_precision /\n",
    "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0585c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輔助函式\n",
    "# 計算句子檢索的招回率 recall\n",
    "def evidence_macro_recall(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate recall for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of recall)\n",
    "        [2]: relevant (denominator of recall)\n",
    "    \"\"\"\n",
    "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # If there's no evidence to predict, return 1\n",
    "        if len(instance[\"evidence\"]) == 0 or all(\n",
    "            [len(eg) == 0 for eg in instance]):\n",
    "            return 1.0, 1.0\n",
    "        claim = instance[\"claim\"]\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "        for evidence_group in instance[\"evidence\"]:\n",
    "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
    "            if all([item in predicted_evidence for item in evidence]):\n",
    "                # We only want to score complete groups of evidence. Incomplete\n",
    "                # groups are worthless.\n",
    "                return 1.0, 1.0\n",
    "        return 0.0, 1.0\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64cb8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算句子檢索的分數\n",
    "def evaluate_retrieval(\n",
    "    probs: np.ndarray,\n",
    "    df_evidences: pd.DataFrame,\n",
    "    ground_truths: pd.DataFrame,\n",
    "    top_n: int = 5,\n",
    "    cal_scores: bool = True,\n",
    "    save_name: str = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate the scores of sentence retrieval\n",
    "\n",
    "    Args:\n",
    "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
    "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
    "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
    "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: F1 score, precision, and recall\n",
    "    \"\"\"\n",
    "    df_evidences[\"prob\"] = probs\n",
    "    top_rows = (\n",
    "        df_evidences.groupby(\"claim\").apply(\n",
    "        lambda x: x.nlargest(top_n, \"prob\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if cal_scores:\n",
    "        macro_precision = 0\n",
    "        macro_precision_hits = 0\n",
    "        macro_recall = 0\n",
    "        macro_recall_hits = 0\n",
    "\n",
    "        for i, instance in enumerate(ground_truths):\n",
    "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
    "            macro_precision += macro_prec[0]\n",
    "            macro_precision_hits += macro_prec[1]\n",
    "\n",
    "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
    "            macro_recall += macro_rec[0]\n",
    "            macro_recall_hits += macro_rec[1]\n",
    "\n",
    "        pr = (macro_precision /\n",
    "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
    "        rec = (macro_recall /\n",
    "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
    "        f1 = 2.0 * pr * rec / (pr + rec)\n",
    "\n",
    "    if save_name is not None:\n",
    "        # write doc7_sent5 file\n",
    "        with open(f\"data/{save_name}\", \"w\", encoding='utf-8') as f:\n",
    "            for instance in ground_truths:\n",
    "                claim = instance[\"claim\"]\n",
    "                predicted_evidence = top_rows[\n",
    "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
    "                instance[\"predicted_evidence\"] = predicted_evidence\n",
    "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if cal_scores:\n",
    "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22a7c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論腳本以獲取候選證據句子的概率\n",
    "def get_predicted_probs(\n",
    "    model: nn.Module,\n",
    "    dataloader: Dataset,\n",
    "    device: torch.device,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Inference script to get probabilites for the candidate evidence sentences\n",
    "\n",
    "    Args:\n",
    "        model: the one from HuggingFace Transformers\n",
    "        dataloader: devset or testset in torch dataloader\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: probabilites of the candidate evidence sentences\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probs.extend(torch.softmax(logits, dim=1)[:, 1].tolist())\n",
    "\n",
    "    return np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a3d58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentRetrievalBERTDataset(BERTDataset):\n",
    "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        sentA = item[\"claim\"]\n",
    "        sentB = item[\"text\"]\n",
    "\n",
    "        # claim [SEP] text\n",
    "        concat = self.tokenizer(\n",
    "            sentA,\n",
    "            sentB,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(item[\"label\"])\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "378255b6-4571-4e76-83ec-ab0c9fa7bd45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def pair_with_wiki_sentences(\n",
    "#     mapping: Dict[str, Dict[int, str]],\n",
    "#     df: pd.DataFrame,\n",
    "#     negative_ratio: float,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Only for creating train sentences.\"\"\"\n",
    "#     data = []\n",
    "\n",
    "#     # positive\n",
    "#     for i in range(len(df)):\n",
    "#         if df[\"label\"].iloc[i] != \"NOT ENOUGH INFO\":\n",
    "#             claim = df[\"claim\"].iloc[i]\n",
    "#             evidence_sets = df[\"evidence\"].iloc[i]\n",
    "#             for evidence_set in evidence_sets:\n",
    "#                 sents = [mapping[evidence[2].replace(\" \", \"_\")][str(evidence[3])] for evidence in evidence_set if evidence[2].replace(\" \", \"_\") != \"臺灣海峽危機#第二次臺灣海峽危機（1958）\"]\n",
    "#                 whole_evidence = \" \".join(sents)\n",
    "#                 data.append((claim, whole_evidence, 1))\n",
    "\n",
    "#     # negative\n",
    "#     for i in range(len(df)):\n",
    "#         if df[\"label\"].iloc[i] != \"NOT ENOUGH INFO\":\n",
    "#             claim = df[\"claim\"].iloc[i]\n",
    "#             evidence_set = {(evidence[2], evidence[3]) for evidences in df[\"evidence\"][i] for evidence in evidences}\n",
    "#             predicted_pages = df[\"predicted_pages\"][i]\n",
    "#             for page in predicted_pages:\n",
    "#                 page = page.replace(\" \", \"_\")\n",
    "#                 try:\n",
    "#                     page_sent_id_pairs = [(page, sent_idx) for sent_idx in mapping[page].keys()]\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "\n",
    "#                 for pair in page_sent_id_pairs:\n",
    "#                     if pair not in evidence_set:\n",
    "#                         text = mapping[page][pair[1]]\n",
    "#                         if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
    "#                             data.append((claim, text, 0))\n",
    "\n",
    "#     return pd.DataFrame(data, columns=[\"claim\", \"text\", \"label\"])\n",
    "\n",
    "# def pair_with_wiki_sentences_eval(\n",
    "#     mapping: Dict[str, Dict[int, str]],\n",
    "#     df: pd.DataFrame,\n",
    "#     is_testset: bool = False,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Only for creating dev and test sentences.\"\"\"\n",
    "#     data = []\n",
    "\n",
    "#     # negative\n",
    "#     for i in range(len(df)):\n",
    "#         claim = df[\"claim\"].iloc[i]\n",
    "#         predicted_pages = df[\"predicted_pages\"][i]\n",
    "#         for page in predicted_pages:\n",
    "#             page = page.replace(\" \", \"_\")\n",
    "#             try:\n",
    "#                 page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "\n",
    "#             for page_name, sentence_id in page_sent_id_pairs:\n",
    "#                 text = mapping[page][sentence_id]\n",
    "#                 if text != \"\":\n",
    "#                     if not is_testset:\n",
    "#                         evidence = df.loc[i, \"evidence\"]\n",
    "#                         data.append((claim, text, evidence))\n",
    "#                     else:\n",
    "#                         data.append((claim, text))\n",
    "\n",
    "#     columns = [\"claim\", \"text\"]\n",
    "#     if not is_testset:\n",
    "#         columns.append(\"evidence\")\n",
    "\n",
    "#     return pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4aa9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原function\n",
    "def pair_with_wiki_sentences(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    negative_ratio: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating train sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # positive\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        evidence_sets = df[\"evidence\"].iloc[i]\n",
    "        for evidence_set in evidence_sets:\n",
    "            sents = []\n",
    "            for evidence in evidence_set:\n",
    "                # evidence[2] is the page title\n",
    "                page = evidence[2].replace(\" \", \"_\")\n",
    "                # the only page with weird name\n",
    "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                    continue\n",
    "                # evidence[3] is in form of int however, mapping requires str\n",
    "                sent_idx = str(evidence[3])\n",
    "                sents.append(mapping[page][sent_idx])\n",
    "\n",
    "            whole_evidence = \" \".join(sents)\n",
    "\n",
    "            claims.append(claim)\n",
    "            sentences.append(whole_evidence)\n",
    "            labels.append(1)\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        evidence_set = set([(evidence[2], evidence[3])\n",
    "                            for evidences in df[\"evidence\"][i]\n",
    "                            for evidence in evidences])\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [\n",
    "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
    "                ]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for pair in page_sent_id_pairs:\n",
    "                if pair in evidence_set:\n",
    "                    continue\n",
    "                text = mapping[page][pair[1]]\n",
    "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
    "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text)\n",
    "                    labels.append(0)\n",
    "\n",
    "    return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"label\": labels})\n",
    "\n",
    "def pair_with_wiki_sentences_eval(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    is_testset: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    evidence = []\n",
    "    predicted_evidence = []\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "        #     continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for page_name, sentence_id in page_sent_id_pairs:\n",
    "                text = mapping[page][sentence_id]\n",
    "                if text != \"\":\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text)\n",
    "                    if not is_testset:\n",
    "                        evidence.append(df.loc[i, \"evidence\"])\n",
    "                    #evidence.append(df[\"evidence\"].iloc[i])\n",
    "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"claim\": claims,\n",
    "        \"text\": sentences,\n",
    "        \"evidence\": evidence if not is_testset else None,\n",
    "        \"predicted_evidence\": predicted_evidence,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa1d3c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a61eefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一個模型 \n",
    "# Sentence Retrieval\n",
    "\n",
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "MODEL_NAME = \"sijunhe/nezha-base-wwm\"  #@param {type:\"string\"}\n",
    "NUM_EPOCHS = 1  #@param {type:\"integer\"}\n",
    "LR = 6e-5  #@param {type:\"number\"}\n",
    "TRAIN_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "TEST_BATCH_SIZE = 256  #@param {type:\"integer\"}\n",
    "# Negative_ration 正負例子的比例\n",
    "NEGATIVE_RATIO = 0.031  #@param {type:\"number\"}\n",
    "VALIDATION_STEP = 150  #@param {type:\"integer\"}\n",
    "# TOP_N 候選證據句子數量，指定每個輸入樣本預測的候選證據句子數量。\n",
    "# 較大的值表示保留更多的候選證據句子，但可能增加計算成本。\n",
    "TOP_N = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23c7b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = f\"sent_retrieval/e{NUM_EPOCHS}_0601_1148{MODEL_NAME}{TRAIN_BATCH_SIZE}_\" + f\"{LR}_neg{NEGATIVE_RATIO}_top{TOP_N}\"\n",
    "LOG_DIR = \"logs/\" + EXP_DIR\n",
    "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
    "\n",
    "if not Path(LOG_DIR).exists():\n",
    "    Path(LOG_DIR).mkdir(parents=True)\n",
    "\n",
    "if not Path(CKPT_DIR).exists():\n",
    "    Path(CKPT_DIR).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77d7f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using the following train data with 0 (Negative) and 1 (Positive)\n",
      "0    7217\n",
      "1    7079\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pair_with_wiki_sentences(\n",
    "    mapping,\n",
    "    pd.DataFrame(TRAIN_GT),\n",
    "    NEGATIVE_RATIO,\n",
    ")\n",
    "# print(TRAIN_GT)\n",
    "counts = train_df[\"label\"].value_counts()\n",
    "print(\"Now using the following train data with 0 (Negative) and 1 (Positive)\")\n",
    "print(counts)\n",
    "\n",
    "dev_evidences = pair_with_wiki_sentences_eval(mapping, pd.DataFrame(DEV_GT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51fcc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, user_faset=True)\n",
    "\n",
    "train_dataset = SentRetrievalBERTDataset(train_df, tokenizer=tokenizer)\n",
    "val_dataset = SentRetrievalBERTDataset(dev_evidences, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32a67a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d22958be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開啟GPU運算\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ae17506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95bf0e8e5fa426a8f82770cc5b410b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f810fc52b869441ba5aaad0125c6cd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.3724839309687392, 'Precision': 0.24785042991401243, 'Recall': 0.749250149970006}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4df2691040468e9431cab54537c5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.37388905218895857, 'Precision': 0.2485702859428067, 'Recall': 0.7540491901619676}\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "# 訓練跑進度條\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "current_steps = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "\n",
    "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        y_true = batch[\"labels\"].tolist()\n",
    "\n",
    "        current_steps += 1\n",
    "\n",
    "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "            print(\"Start validation\")\n",
    "            probs = get_predicted_probs(model, eval_dataloader, device)\n",
    "\n",
    "            val_results = evaluate_retrieval(\n",
    "                probs=probs,\n",
    "                df_evidences=dev_evidences,\n",
    "                ground_truths=DEV_GT,\n",
    "                top_n=TOP_N,\n",
    "            )\n",
    "            print(val_results)\n",
    "\n",
    "            # log each metric separately to TensorBoard\n",
    "            for metric_name, metric_value in val_results.items():\n",
    "                writer.add_scalar(\n",
    "                    f\"dev_{metric_name}\",\n",
    "                    metric_value,\n",
    "                    current_steps,\n",
    "                )\n",
    "\n",
    "            save_checkpoint(model, CKPT_DIR, current_steps)\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dab1ba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23172), started 6 days, 14:58:13 ago. (Use '!kill 23172' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7ce90b953d30b92d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7ce90b953d30b92d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f48a0e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start final evaluations and write prediction files.\n",
      "Start calculating training scores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab75052dce8f4e25be4c389209fe9138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores => {'F1 score': 0.3888014888167915, 'Precision': 0.26218237294919955, 'Recall': 0.7519507803121248}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc98f26fa594b378ef6ca18c3636677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores => {'F1 score': 0.37388905218895857, 'Precision': 0.2485702859428067, 'Recall': 0.7540491901619676}\n"
     ]
    }
   ],
   "source": [
    "# 驗證訓練結果\n",
    "ckpt_name = \"model.300.pt\"  #@param {type:\"string\"}\n",
    "model = load_model(model, ckpt_name, CKPT_DIR)\n",
    "print(\"Start final evaluations and write prediction files.\")\n",
    "\n",
    "train_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping=mapping,\n",
    "    df=pd.DataFrame(TRAIN_GT),\n",
    ")\n",
    "train_set = SentRetrievalBERTDataset(train_evidences, tokenizer)\n",
    "train_dataloader = DataLoader(train_set, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "print(\"Start calculating training scores\")\n",
    "probs = get_predicted_probs(model, train_dataloader, device)\n",
    "train_results = evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=train_evidences,\n",
    "    ground_truths=TRAIN_GT,\n",
    "    top_n=TOP_N,\n",
    "    save_name=f\"train_doc5sent{TOP_N}.jsonl\",\n",
    ")\n",
    "print(f\"Training scores => {train_results}\")\n",
    "\n",
    "print(\"Start validation\")\n",
    "probs = get_predicted_probs(model, eval_dataloader, device)\n",
    "val_results = evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=dev_evidences,\n",
    "    ground_truths=DEV_GT,\n",
    "    top_n=TOP_N,\n",
    "    save_name=f\"dev_doc5sent{TOP_N}.jsonl\",\n",
    ")\n",
    "\n",
    "print(f\"Validation scores => {val_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a86d2315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting the test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959b50d2631a4881a159efaaec8969d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = load_json(\"data/test_doc5.jsonl\")\n",
    "\n",
    "test_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping,\n",
    "    pd.DataFrame(test_data),\n",
    "    is_testset=True,\n",
    ")\n",
    "test_set = SentRetrievalBERTDataset(test_evidences, tokenizer)\n",
    "test_dataloader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "print(\"Start predicting the test data\")\n",
    "probs = get_predicted_probs(model, test_dataloader, device)\n",
    "evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=test_evidences,\n",
    "    ground_truths=test_data,\n",
    "    top_n=TOP_N,\n",
    "    cal_scores=False,\n",
    "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4214127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f1a070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全域變數設置\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/train_doc5sent5.jsonl\")\n",
    "DEV_DATA = load_json(\"data/dev_doc5sent5.jsonl\")\n",
    "\n",
    "TRAIN_PKL_FILE = Path(\"data/train_doc5sent5.pkl\")\n",
    "DEV_PKL_FILE = Path(\"data/dev_doc5sent5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4dda097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0b45dbd8904474be6c3a13ca74df06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=296938), Label(value='0 / 296938')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
    "del wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d969706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
    "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        claim = item[\"claim\"]\n",
    "        evidence = item[\"evidence_list\"]\n",
    "\n",
    "        # In case there are less than topk evidence sentences\n",
    "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
    "        evidence += pad\n",
    "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
    "\n",
    "        concat = self.tokenizer(\n",
    "            concat_claim_evidence,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(label)\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c2ff8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            y_true.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "37f73579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = model(**batch).logits\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        preds.extend(pred.tolist())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9f1e1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    if mode == \"eval\":\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "        print(df[\"evidence_list\"][:5])\n",
    "    else:\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
    "            \" \".join([  # join evidence\n",
    "                mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "                for _, _, evi_id, evi_idx in evi_list\n",
    "            ]) if isinstance(evi_list, list) else \"\"\n",
    "            for evi_list in x  # for each evidence list\n",
    "        ][:1] if isinstance(x, list) else [])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "85f682a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二個模型 \n",
    "# Claim Verification\n",
    "#將證據和 claim 丟入 模型判斷正確錯誤或資訊缺乏\n",
    "\n",
    "#@title  { display-mode: \"form\" }\n",
    "# bert-base-chinese 原本模型\n",
    "MODEL_NAME = \"sijunhe/nezha-base-wwm\"  #@param {type:\"string\"}\n",
    "TRAIN_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "TEST_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "SEED = 1024  #@param {type:\"integer\"}\n",
    "LR = 5e-5  #@param {type:\"number\"}\n",
    "NUM_EPOCHS = 30  #@param {type:\"integer\"}\n",
    "MAX_SEQ_LEN = 256  #@param {type:\"integer\"}\n",
    "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}\n",
    "VALIDATION_STEP = 50  #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "597c6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = \"submission.jsonl\"\n",
    "# 原本 _bs{TRAIN_BATCH_SIZE}_\n",
    "EXP_DIR = f\"claim_verification/e{NUM_EPOCHS}_test50_1223_new{MODEL_NAME}_{TRAIN_BATCH_SIZE}_{TEST_BATCH_SIZE}\" + f\"{LR}_top{EVIDENCE_TOPK}\"\n",
    "LOG_DIR = \"logs/\" + EXP_DIR\n",
    "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
    "\n",
    "if not Path(LOG_DIR).exists():\n",
    "    Path(LOG_DIR).mkdir(parents=True)\n",
    "\n",
    "if not Path(CKPT_DIR).exists():\n",
    "    Path(CKPT_DIR).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f6cebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_PKL_FILE.exists():\n",
    "    train_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TRAIN_DATA),\n",
    "        mapping,\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TRAIN_PKL_FILE, \"rb\") as f:\n",
    "        train_df = pickle.load(f)\n",
    "\n",
    "if not DEV_PKL_FILE.exists():\n",
    "    dev_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(DEV_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    dev_df.to_pickle(DEV_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(DEV_PKL_FILE, \"rb\") as f:\n",
    "        dev_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5e484c3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_allocated\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m      3\u001b[0m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU memory is fully cleared.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\memory.py:330\u001b[0m, in \u001b[0;36mmemory_allocated\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmemory_allocated\u001b[39m(device: Union[Device, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the current GPU memory occupied by tensors in bytes for a given\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    device.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m        details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmemory_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallocated_bytes.all.current\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\memory.py:210\u001b[0m, in \u001b[0;36mmemory_stats\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    207\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend((prefix, obj))\n\u001b[0;32m    209\u001b[0m stats \u001b[38;5;241m=\u001b[39m memory_stats_as_nested_dict(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 210\u001b[0m \u001b[43m_recurse_add_to_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m result\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collections\u001b[38;5;241m.\u001b[39mOrderedDict(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\memory.py:205\u001b[0m, in \u001b[0;36mmemory_stats.<locals>._recurse_add_to_result\u001b[1;34m(prefix, obj)\u001b[0m\n\u001b[0;32m    203\u001b[0m         prefix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 205\u001b[0m         \u001b[43m_recurse_add_to_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend((prefix, obj))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\memory.py:205\u001b[0m, in \u001b[0;36mmemory_stats.<locals>._recurse_add_to_result\u001b[1;34m(prefix, obj)\u001b[0m\n\u001b[0;32m    203\u001b[0m         prefix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 205\u001b[0m         \u001b[43m_recurse_add_to_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend((prefix, obj))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\memory.py:205\u001b[0m, in \u001b[0;36mmemory_stats.<locals>._recurse_add_to_result\u001b[1;34m(prefix, obj)\u001b[0m\n\u001b[0;32m    203\u001b[0m         prefix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 205\u001b[0m         \u001b[43m_recurse_add_to_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend((prefix, obj))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "while torch.cuda.memory_allocated() > 5:\n",
    "      pass\n",
    "print(\"GPU memory is fully cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a76c6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "  BertTokenizerFast,\n",
    "  AutoModel,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "val_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "08974cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABEL2ID),\n",
    ")\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c56aeed-1c84-45e9-8f7f-d47cebed7c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "# current_steps = 0\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     model.train()\n",
    "\n",
    "#     for batch in train_dataloader:\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "#         with autocast():\n",
    "#             outputs = model(**batch)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         progress_bar.update(1)\n",
    "#         writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "\n",
    "#         y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "#         y_true = batch[\"labels\"].tolist()\n",
    "\n",
    "#         current_steps += 1\n",
    "\n",
    "#         if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "#             print(\"Start validation\")\n",
    "#             val_results = run_evaluation(model, eval_dataloader, device)\n",
    "\n",
    "#             # log each metric separately to TensorBoard\n",
    "#             for metric_name, metric_value in val_results.items():\n",
    "#                 print(f\"{metric_name}: {metric_value}\")\n",
    "#                 writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
    "\n",
    "#             save_checkpoint(\n",
    "#                 model,\n",
    "#                 CKPT_DIR,\n",
    "#                 current_steps,\n",
    "#                 mark=f\"val_acc={val_results['val_acc']:.7f}\",\n",
    "#             )\n",
    "\n",
    "# print(\"Finished training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8138c16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9b41db392f44adaf7ad0fef19d4d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da96f52c86e949d1a6f3c0771772d84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.0841463902225232\n",
      "val_acc: 0.4270386266094421\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cbc1720cfe427f8f3b427e3dc4b9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.1347969574471042\n",
      "val_acc: 0.4313304721030043\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d7166229584bf3b29fdd85c21c6193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.2339141956747395\n",
      "val_acc: 0.43261802575107294\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8510903aa2d40eab0e40596a1ca2689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.42541016454566\n",
      "val_acc: 0.43047210300429184\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee80c8bef874bcfad0b3fb8df532ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.542861226486833\n",
      "val_acc: 0.4236051502145923\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e83b193528a425dadaf0c37e0cce7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.7586792299192247\n",
      "val_acc: 0.42832618025751074\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7bb0e36643471ba5f339b5e49004e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.8471560584355706\n",
      "val_acc: 0.4369098712446352\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f9f2ef64d54c2c8ecc0ad9b5923bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.852574698729058\n",
      "val_acc: 0.47296137339055794\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36838e6c43ff4beaa765c57ed39eba96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.036146578723437\n",
      "val_acc: 0.4493562231759657\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a994fbf3022f4be2b69adbf25ff15d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.9359408617019653\n",
      "val_acc: 0.48025751072961376\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3fdcd13cc6489d9580e296229e9fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.179202272467417\n",
      "val_acc: 0.4682403433476395\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3469ce79df724fbba4da13005822748a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.0374499020511156\n",
      "val_acc: 0.48412017167381977\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc29c0eb07c406a87edfbd246c1897f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.3178424051363176\n",
      "val_acc: 0.463519313304721\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf243349001e436bae06beeca7f70e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.209486752340238\n",
      "val_acc: 0.4781115879828326\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0c862d8a6a4ed59b339a09cfcc5750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.3445157972100663\n",
      "val_acc: 0.47639484978540775\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c35dd5d3c894c1f8ec58293ac05723a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.3142970424808866\n",
      "val_acc: 0.4854077253218884\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eaa72e8c68484790dd102c050b0ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.479110934962965\n",
      "val_acc: 0.47296137339055794\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbb2482a421475da921c004fdda6ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.464578388488456\n",
      "val_acc: 0.4918454935622318\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4054fb0ef646ecb68953f287ed416a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.7198348224979556\n",
      "val_acc: 0.4742489270386266\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc903cdb177452ea7c73d3cfe817160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.595059786757378\n",
      "val_acc: 0.4901287553648069\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b723d3cf7547498b120da1c0217031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.8140985345187253\n",
      "val_acc: 0.45107296137339054\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a90406d6074565a8a3c654b04d5701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.6404261033828944\n",
      "val_acc: 0.496137339055794\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ca9b8c0fb94a8d86d698e941e9d9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.634589730876766\n",
      "val_acc: 0.4918454935622318\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf46d00f30af4badb4b4007c3af2653f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.6182128403284777\n",
      "val_acc: 0.492274678111588\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158d3b1252be473c80e377d5d2da17a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.699944695381269\n",
      "val_acc: 0.4957081545064378\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d63ec7d8a74caead319b6308c97eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.8466658918824916\n",
      "val_acc: 0.48626609442060087\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d898020c114945bad6b409eede400d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.883146931047309\n",
      "val_acc: 0.4648068669527897\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b55a38968e546169230324c21e53d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.7772392181500996\n",
      "val_acc: 0.4738197424892704\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e123949b3af44f3eb3ec0c4a097635f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.717853221174789\n",
      "val_acc: 0.49527896995708154\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76f4baa93754803afae3c97cc44909f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.7134608552880484\n",
      "val_acc: 0.49141630901287553\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c172e3f2d18476c8a1ff25ada579458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.8270367775877863\n",
      "val_acc: 0.496137339055794\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8403354a316c4c3d9ebd0b03d19c6b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.9335894960246676\n",
      "val_acc: 0.4957081545064378\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d2eee6ac2545228e7d4bdbcbfada00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.8655326235784244\n",
      "val_acc: 0.5051502145922747\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3662e7bf69427f8e4870ee1dea98c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.9609367406531555\n",
      "val_acc: 0.4927038626609442\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba08cdbd02ab45fa8f945103b25ce0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.0767986937744976\n",
      "val_acc: 0.48068669527896996\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f664aa8887d04accbf93230c5a9da100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.013879281200775\n",
      "val_acc: 0.4858369098712446\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cae7677a6b431db2d24d2cf346bed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.040995330026705\n",
      "val_acc: 0.4957081545064378\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857c9d46e8dc44a29a60be9745d0c3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.0021217963466906\n",
      "val_acc: 0.4995708154506438\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4195f6e49f94d85a9fd214eb12b1841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.891991373610823\n",
      "val_acc: 0.47339055793991414\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36502e6648394e079093fa309cbf31cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.7113682946113693\n",
      "val_acc: 0.5034334763948498\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0945f3b233df4459bf89de09a6483889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.050610259787677\n",
      "val_acc: 0.4918454935622318\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22724d6daca8436185dd1c872ece008b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.06031704112275\n",
      "val_acc: 0.49914163090128755\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e072c1739dc4cdba594ecf535bb76be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.8965122928358102\n",
      "val_acc: 0.4982832618025751\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da4226b36c047ac9b36adc6bcc8269a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.888505380447597\n",
      "val_acc: 0.4939914163090129\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c5de29a5024ae4bb750f9eafe78701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.1024085854830807\n",
      "val_acc: 0.4918454935622318\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d532930f75744b2937d821074383d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.9438475958288532\n",
      "val_acc: 0.5025751072961373\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4dbdd12d504f03a8afff679fbe8878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.04124827091008\n",
      "val_acc: 0.5025751072961373\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4e5e5127f34dc78c4ef9bfb6c91631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.148745407796886\n",
      "val_acc: 0.4939914163090129\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebad8ba5a5142399e0571aa559dfa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.271685347165147\n",
      "val_acc: 0.4939914163090129\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aace15b7eba4ef7b835033da634d91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.275198653952716\n",
      "val_acc: 0.49699570815450644\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db82f148111846e79a9636678d1106a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.129294609370297\n",
      "val_acc: 0.5008583690987124\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b8aabc917549189b528e0e37340fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.232289250582865\n",
      "val_acc: 0.5111587982832618\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2de669e4044756845c0fe6aee22562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.44342029911198\n",
      "val_acc: 0.49871244635193135\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c1b442e299482ca723ab34078cbd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.5277858041737176\n",
      "val_acc: 0.4832618025751073\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bb5c9c6b8d4548a8e65a98cf1f2c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.497945586295977\n",
      "val_acc: 0.48669527896995707\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c100f24643f431dadc0488c7c81c387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.4374903865056496\n",
      "val_acc: 0.49742489270386264\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735cba8d184444e097310d0487cf6501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.2584721633832747\n",
      "val_acc: 0.5025751072961373\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86f2da24e464a05a07dc75f35f04b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.3095914941944486\n",
      "val_acc: 0.4957081545064378\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab502c3b90b45fa89016ff9f78ec1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.552575735196675\n",
      "val_acc: 0.5004291845493563\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a88a07e22d146879fd5da12c41c032f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.6763370037078857\n",
      "val_acc: 0.488412017167382\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1715aaddcf4a9a9023e05230538ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.8309535082072426\n",
      "val_acc: 0.47510729613733904\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a296dc5c0e194c8595e76b878c83ebef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.5186003805839854\n",
      "val_acc: 0.49699570815450644\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36114073e2e44ac584653adba60e652b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.696123867818754\n",
      "val_acc: 0.48454935622317596\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baa4ad7f94f41088a2f80441cfa139a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.3922827096834576\n",
      "val_acc: 0.48454935622317596\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e4431c095f4bd0aa23f27791005c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.6285011245779795\n",
      "val_acc: 0.48884120171673817\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eec9537d4e44e29bea2f1ec35829dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.663491916983095\n",
      "val_acc: 0.49699570815450644\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c80405bb343688e735e89b27329e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.550302420576958\n",
      "val_acc: 0.4896995708154506\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391b904644a44503a1b2c2907ee8da34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.6403930334195698\n",
      "val_acc: 0.49141630901287553\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848e2b6e21684adfaede8c5e118a29ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.702002526962594\n",
      "val_acc: 0.49141630901287553\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998a5a712d294eca8146d5501b6ca537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.5918655085237057\n",
      "val_acc: 0.4995708154506438\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392fb9d01dd24e00af1e3412ff69237e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.7811729271117955\n",
      "val_acc: 0.49527896995708154\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05302dcea7c4aaa9d1597ad3c7a7cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.7480041588822455\n",
      "val_acc: 0.49699570815450644\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\caffe2\\serialize\\inline_container.cc:325] . unexpected pos 726114368 vs 726114256",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:423\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 423\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:650\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    649\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[1;32m--> 650\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\caffe2\\serialize\\inline_container.cc:450] . PytorchStreamWriter failed writing file data/202: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m                 writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, metric_value, current_steps)\n\u001b[1;32m---> 33\u001b[0m             \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43mCKPT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcurrent_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_acc=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mval_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.7f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\baseline\\utils.py:124\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[1;34m(model, ckpt_dir, current_step, mark)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mark \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    123\u001b[0m     mark \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 124\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmark\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mmodel.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_step\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:290\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\caffe2\\serialize\\inline_container.cc:325] . unexpected pos 726114368 vs 726114256"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "current_steps = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "\n",
    "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        y_true = batch[\"labels\"].tolist()\n",
    "\n",
    "        current_steps += 1\n",
    "\n",
    "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "            print(\"Start validation\")\n",
    "            val_results = run_evaluation(model, eval_dataloader, device)\n",
    "\n",
    "            # log each metric separately to TensorBoard\n",
    "            for metric_name, metric_value in val_results.items():\n",
    "                print(f\"{metric_name}: {metric_value}\")\n",
    "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
    "\n",
    "            save_checkpoint(\n",
    "                model,\n",
    "                CKPT_DIR,\n",
    "                current_steps,\n",
    "                mark=f\"val_acc={val_results['val_acc']:.7f}\",\n",
    "            )\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dcd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
    "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
    "\n",
    "if not TEST_PKL_FILE.exists():\n",
    "    test_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TEST_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TEST_PKL_FILE, \"rb\") as f:\n",
    "        test_df = pickle.load(f)\n",
    "\n",
    "test_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = \"val_acc=0.5266094_model.6500.pt\"  #@param {type:\"string\"}\n",
    "model = load_model(model, ckpt_name, CKPT_DIR)\n",
    "predicted_label = run_predict(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38003552",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = test_df.copy()\n",
    "predict_dataset[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label))\n",
    "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
    "    OUTPUT_FILENAME,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "while torch.cuda.memory_allocated() > 5:\n",
    "    pass\n",
    "print(\"GPU memory is fully cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c805a2-6402-4abf-9926-409c63571a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
